{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.downloader import create_local_data, download_source_data\n",
    "from codes.parameters import categories\n",
    "category = categories[0] # \"All_Beauty\"\n",
    "# download_source_data(category)\n",
    "# create_local_data(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.filter import filter\n",
    "data = filter(category = category, min_length = 3, min_reviews_per_product = 5, min_average_rating = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This spray is really nice. It smells really go...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This product does what I need it to do, I just...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smells good, feels great!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The polish was quiet thick and did not apply s...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>These were lightweight and soft but much too s...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>This is a cute headband and is very comfortabl...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13313</th>\n",
       "      <td>This is an honest non paid for non free item g...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13315</th>\n",
       "      <td>Well made and good assortment.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13316</th>\n",
       "      <td>Iâ€™m a barber here throwing my two cents, just ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>My teeth are very closely spaced so I have dif...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  rating\n",
       "0      This spray is really nice. It smells really go...     5.0\n",
       "1      This product does what I need it to do, I just...     4.0\n",
       "2                              Smells good, feels great!     5.0\n",
       "5      The polish was quiet thick and did not apply s...     4.0\n",
       "7      These were lightweight and soft but much too s...     3.0\n",
       "...                                                  ...     ...\n",
       "13312  This is a cute headband and is very comfortabl...     2.0\n",
       "13313  This is an honest non paid for non free item g...     4.0\n",
       "13315                     Well made and good assortment.     5.0\n",
       "13316  Iâ€™m a barber here throwing my two cents, just ...     5.0\n",
       "13317  My teeth are very closely spaced so I have dif...     5.0\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[['text','rating']].head(10000)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(sample.text)\n",
    "labels = [int(x)-1 for x in sample.rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\MichaÅ‚\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\MichaÅ‚\\AppData\\Local\\Temp\\ipykernel_14612\\4204561637.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469342edd26f431782821c2f018b41eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6264, 'grad_norm': 6.784568786621094, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3162, 'grad_norm': 6.3815484046936035, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.04}\n",
      "{'loss': 1.3521, 'grad_norm': 6.8445820808410645, 'learning_rate': 1.88e-05, 'epoch': 0.06}\n",
      "{'loss': 1.2951, 'grad_norm': 3.634737730026245, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.08}\n",
      "{'loss': 1.0987, 'grad_norm': 7.815173625946045, 'learning_rate': 1.8e-05, 'epoch': 0.1}\n",
      "{'loss': 1.2681, 'grad_norm': 5.619175910949707, 'learning_rate': 1.76e-05, 'epoch': 0.12}\n",
      "{'loss': 1.0667, 'grad_norm': 3.3037185668945312, 'learning_rate': 1.72e-05, 'epoch': 0.14}\n",
      "{'loss': 1.0021, 'grad_norm': 4.422407150268555, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 1.2395, 'grad_norm': 5.572620868682861, 'learning_rate': 1.64e-05, 'epoch': 0.18}\n",
      "{'loss': 0.9695, 'grad_norm': 7.00856876373291, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.2}\n",
      "{'loss': 0.9457, 'grad_norm': 4.317888259887695, 'learning_rate': 1.5600000000000003e-05, 'epoch': 0.22}\n",
      "{'loss': 0.9319, 'grad_norm': 4.399068355560303, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.24}\n",
      "{'loss': 0.9787, 'grad_norm': 6.061038017272949, 'learning_rate': 1.48e-05, 'epoch': 0.26}\n",
      "{'loss': 0.9334, 'grad_norm': 7.338369369506836, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.28}\n",
      "{'loss': 0.914, 'grad_norm': 6.787537574768066, 'learning_rate': 1.4e-05, 'epoch': 0.3}\n",
      "{'loss': 1.0151, 'grad_norm': 13.443117141723633, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.32}\n",
      "{'loss': 0.8089, 'grad_norm': 5.06701135635376, 'learning_rate': 1.3200000000000002e-05, 'epoch': 0.34}\n",
      "{'loss': 0.8671, 'grad_norm': 8.11548900604248, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.36}\n",
      "{'loss': 0.9096, 'grad_norm': 4.496811866760254, 'learning_rate': 1.2400000000000002e-05, 'epoch': 0.38}\n",
      "{'loss': 0.9696, 'grad_norm': 6.885212421417236, 'learning_rate': 1.2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7722, 'grad_norm': 4.00639009475708, 'learning_rate': 1.16e-05, 'epoch': 0.42}\n",
      "{'loss': 0.8195, 'grad_norm': 4.6371684074401855, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 0.7277, 'grad_norm': 8.025659561157227, 'learning_rate': 1.0800000000000002e-05, 'epoch': 0.46}\n",
      "{'loss': 0.8359, 'grad_norm': 8.986008644104004, 'learning_rate': 1.04e-05, 'epoch': 0.48}\n",
      "{'loss': 0.7777, 'grad_norm': 4.3102216720581055, 'learning_rate': 1e-05, 'epoch': 0.5}\n",
      "{'loss': 0.8024, 'grad_norm': 12.445384979248047, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.52}\n",
      "{'loss': 0.8564, 'grad_norm': 4.269680976867676, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.54}\n",
      "{'loss': 0.7555, 'grad_norm': 6.914539813995361, 'learning_rate': 8.8e-06, 'epoch': 0.56}\n",
      "{'loss': 0.8985, 'grad_norm': 7.432784080505371, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.58}\n",
      "{'loss': 0.7989, 'grad_norm': 8.337299346923828, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 0.8108, 'grad_norm': 4.4916672706604, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.62}\n",
      "{'loss': 0.7916, 'grad_norm': 6.914705276489258, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.64}\n",
      "{'loss': 0.8654, 'grad_norm': 7.02003812789917, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.66}\n",
      "{'loss': 0.84, 'grad_norm': 8.452929496765137, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.68}\n",
      "{'loss': 0.8139, 'grad_norm': 6.1873040199279785, 'learning_rate': 6e-06, 'epoch': 0.7}\n",
      "{'loss': 0.8023, 'grad_norm': 8.185704231262207, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.72}\n",
      "{'loss': 0.8153, 'grad_norm': 4.360240936279297, 'learning_rate': 5.2e-06, 'epoch': 0.74}\n",
      "{'loss': 0.7225, 'grad_norm': 5.987693786621094, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.76}\n",
      "{'loss': 0.7072, 'grad_norm': 7.0905375480651855, 'learning_rate': 4.4e-06, 'epoch': 0.78}\n",
      "{'loss': 0.7425, 'grad_norm': 4.24881649017334, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6933, 'grad_norm': 8.203299522399902, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.82}\n",
      "{'loss': 0.7437, 'grad_norm': 9.489951133728027, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.84}\n",
      "{'loss': 0.8186, 'grad_norm': 4.76323938369751, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.86}\n",
      "{'loss': 0.7453, 'grad_norm': 10.68722915649414, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.88}\n",
      "{'loss': 0.7843, 'grad_norm': 8.4347562789917, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.9}\n",
      "{'loss': 0.7454, 'grad_norm': 6.381653785705566, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.92}\n",
      "{'loss': 0.7634, 'grad_norm': 7.239700794219971, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.94}\n",
      "{'loss': 0.7674, 'grad_norm': 9.361669540405273, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.96}\n",
      "{'loss': 0.8431, 'grad_norm': 10.148748397827148, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.98}\n",
      "{'loss': 0.7844, 'grad_norm': 5.65722131729126, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9958b7ee2d364f79a6509652ce7ef9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.741514265537262, 'eval_accuracy': 0.7055000066757202, 'eval_runtime': 3.8582, 'eval_samples_per_second': 518.379, 'eval_steps_per_second': 64.797, 'epoch': 1.0}\n",
      "{'train_runtime': 55.6724, 'train_samples_per_second': 143.698, 'train_steps_per_second': 8.981, 'train_loss': 0.9030680236816406, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e076e946a64fd2857bb7634d18f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki ewaluacji: {'eval_loss': 0.741514265537262, 'eval_accuracy': 0.7055000066757202, 'eval_runtime': 3.8083, 'eval_samples_per_second': 525.167, 'eval_steps_per_second': 65.646, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "def prepare_dataset(encodings, labels):\n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'labels': labels\n",
    "    })\n",
    "\n",
    "train_dataset = prepare_dataset(train_encodings, train_labels)\n",
    "val_dataset = prepare_dataset(val_encodings, val_labels)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1).to(device)\n",
    "    accuracy = (predictions == torch.tensor(labels).to(device)).float().mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"Wyniki ewaluacji:\", results)\n",
    "\n",
    "\n",
    "trainer.save_model(\"../fine_tuned_bert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WywoÅ‚aÄ‡ tylko raz by pobraÄ‡ potem bÄ™dÄ… problemy bo model juÅ¼ jest zaÅ‚adowany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model i tokenizator zapisane lokalnie.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Pobranie modelu i tokenizatora\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Zapisanie modelu i tokenizatora lokalnie\n",
    "model.save_pretrained(\"./twitter-roberta-sentiment\")\n",
    "tokenizer.save_pretrained(\"./twitter-roberta-sentiment\")\n",
    "print(\"Model i tokenizator zapisane lokalnie.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['positive', 5.0], ['negative', 4.0], ['positive', 5.0], ['negative', 4.0], ['negative', 3.0], ['positive', 5.0], ['positive', 5.0], ['positive', 5.0], ['neutral', 3.0], ['positive', 5.0], ['negative', 3.0], ['positive', 5.0], ['positive', 5.0], ['positive', 5.0], ['positive', 5.0], ['positive', 5.0], ['positive', 5.0], ['negative', 3.0], ['positive', 5.0], ['positive', 5.0]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "size = 20\n",
    "local_model_path = \"./twitter-roberta-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(local_model_path)\n",
    "\n",
    "inputs = tokenizer(list(sample.head(size).text), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Predykcja\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "mapped_predictions = [label_map[label] for label in predictions.tolist()]\n",
    "\n",
    "results = [[pred, val] for pred, val in zip(mapped_predictions, list(sample.head(size).rating))]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WartoÅ›ci: [5, 4, 5, 4, 3, 5, 5, 5, 3, 5, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5]\n",
      "Predykcje: [5, 3, 5, 2, 3, 5, 5, 5, 3, 5, 2, 5, 5, 5, 4, 5, 5, 3, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_path = \"./fine_tuned_bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "inputs = tokenizer(list(sample.head(20).text), padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Przeniesienie danych na GPU (jeÅ›li dostÄ™pne)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "\n",
    "print(\"WartoÅ›ci:\", [int(x) for x in sample.head(20).rating])\n",
    "print(\"Predykcje:\", [x+1 for x in predictions.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
