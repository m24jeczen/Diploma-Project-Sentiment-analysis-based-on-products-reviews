{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sys.path.append('./codes')\n",
    "from codes import LDA \n",
    "from codes import classification\n",
    "import importlib\n",
    "importlib.reload(LDA)\n",
    "importlib.reload(classification)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from codes.LDA import *\n",
    "\n",
    "from codes.classification import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../amazon_data/Amazon_Fashion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "5.0    1411756\n",
       "4.0     348881\n",
       "1.0     300032\n",
       "3.0     243118\n",
       "2.0     171666\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size\n",
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making well-balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "5.0    36030\n",
      "4.0    18004\n",
      "1.0    16521\n",
      "3.0    15569\n",
      "2.0    13876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = df[~df[\"text\"].isna()]\n",
    "data = data[data['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "sampled_per_rating = [data[data['rating'] == rating].sample(n=10000, random_state=42) for rating in range(1, 6)]\n",
    "required_samples = pd.concat(sampled_per_rating, axis=0)\n",
    "remaining_samples = data.drop(required_samples.index).sample(n=100000 - len(required_samples), random_state=42)\n",
    "data = pd.concat([required_samples, remaining_samples], axis=0)\n",
    "data = data[data['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "print(data['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing starting ---\n",
      "---Tokenization done---\n",
      "---Numbers removed---\n",
      "---Two letter words removed---\n",
      "---Written-out numbers removed---\n",
      "---Verbs removed BIG---\n",
      "---Lematization done---\n",
      "---Stopwords removed---\n",
      "---Choosed words removed---\n",
      "---Bigrams done---\n",
      "---Preprocessing done---\n"
     ]
    }
   ],
   "source": [
    "texts_bow, dictionary, id2token = preprocess_text(data, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LDA starting ---\n",
      "--- LDA finished ---\n",
      "Medium koherence of topics: -2.3864.\n",
      "Topic 0: pair, year, old, glass, money, worth, son, year_old, summer, sunglass\n",
      "Topic 1: product, receive, gift, light, item, foot, arrive, bracelet, without, weight\n",
      "Topic 2: time, come, day, use, band, first, lot, work, break, easy\n",
      "Topic 3: fit, love, size, order, buy, small, little, would, perfect, large\n",
      "Topic 4: belt, comfy, boot, second, next, card, weather, first_time, many_compliment, buckle\n",
      "Topic 5: back, say, send, amazon, ship, thank, review, read, seller, costume\n",
      "Topic 6: color, well, make, comfortable, also, much, soft, enough, around, different\n",
      "Topic 7: give, earring, star, ear, dry, real, silver, reason, ankle, shrink\n",
      "Topic 8: great, look, good, quality, nice, really, material, cute, price, purchase\n"
     ]
    }
   ],
   "source": [
    "model, dictionary = LDA_training(data, False,9, 1000, 10, 200, 1, 10, texts_bow, dictionary, id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_sentiment = add_sentiment_column(data)\n",
    "feature_matrix = topic_distributions_to_matrix(model, texts_bow, 9)\n",
    "labels = data_with_sentiment['sentiment_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_spliter(feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Random Forest Training ---\n",
      "Accuracy for Random Forest: 0.5372\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.30      0.35      6084\n",
      "           1       0.20      0.04      0.06      3109\n",
      "           2       0.58      0.82      0.68     10807\n",
      "\n",
      "    accuracy                           0.54     20000\n",
      "   macro avg       0.40      0.38      0.36     20000\n",
      "weighted avg       0.47      0.54      0.48     20000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelR, y_pred = train_and_evaluate_Random_Forest(feature_matrix, labels, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logistic Regression Training ---\n",
      "Accuracy for Logistic Regression: 0.5466\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.06      0.10      6084\n",
      "           1       0.00      0.00      0.00      3109\n",
      "           2       0.55      0.98      0.70     10807\n",
      "\n",
      "    accuracy                           0.55     20000\n",
      "   macro avg       0.36      0.35      0.27     20000\n",
      "weighted avg       0.46      0.55      0.41     20000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "modelL, y_predL = train_and_evaluate_Logistic_Regression(feature_matrix, labels, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking removing the 3 star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "5.0    36030\n",
      "4.0    18004\n",
      "1.0    16521\n",
      "2.0    13876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2 = df[~df[\"text\"].isna()]\n",
    "data2 = data2[data2['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "sampled_per_rating = [data2[data2['rating'] == rating].sample(n=10000, random_state=42) for rating in range(1, 6)]\n",
    "required_samples = pd.concat(sampled_per_rating, axis=0)\n",
    "remaining_samples = data2.drop(required_samples.index).sample(n=100000 - len(required_samples), random_state=42)\n",
    "data2 = pd.concat([required_samples, remaining_samples], axis=0)\n",
    "data2 = data2[data2['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "data2 = data2[data2[\"rating\"]!=3]\n",
    "print(data2['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing starting ---\n",
      "---Tokenization done---\n",
      "---Numbers removed---\n",
      "---Two letter words removed---\n",
      "---Written-out numbers removed---\n",
      "---Verbs removed BIG---\n",
      "---Lematization done---\n",
      "---Stopwords removed---\n",
      "---Choosed words removed---\n",
      "---Bigrams done---\n",
      "---Preprocessing done---\n"
     ]
    }
   ],
   "source": [
    "texts_bow2, dictionary2, id2token2 = preprocess_text(data2, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LDA starting ---\n",
      "--- LDA finished ---\n",
      "Medium koherence of topics: -2.4581.\n",
      "Topic 0: fit, size, order, small, perfect, large, big, way, return, even\n",
      "Topic 1: different, glass, black, white, blue, face, case, sunglass, lens, red\n",
      "Topic 2: year, daughter, perfectly, old, fit_perfectly, foot, year_old, summer, costume, boot\n",
      "Topic 3: wash, dry, cool, christmas, type, glad, house, help, shrink, base\n",
      "Topic 4: quality, price, product, back, item, arrive, mask, amazon, package, send\n",
      "Topic 5: great, look, would, well, color, good, nice, make, really, little\n",
      "Topic 6: love, buy, time, purchase, come, beautiful, receive, gift, lot, easy\n",
      "Topic 7: earring, ring, necklace, belt, ear, lose, chain, metal, jewelry, fast\n",
      "Topic 8: pair, day, use, band, first, work, thing, pocket, try, need\n"
     ]
    }
   ],
   "source": [
    "model2, dictionary2 = LDA_training(data2, False,9, 1000, 10, 200, 1, 10, texts_bow2, dictionary2, id2token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_sentiment2 = add_sentiment_column(data2)\n",
    "feature_matrix2 = topic_distributions_to_matrix(model2, texts_bow2, 9)\n",
    "labels2 = data_with_sentiment2['sentiment_encoded']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_spliter(feature_matrix2, labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Random Forest Training ---\n",
      "Accuracy for Random Forest: 0.6481\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.33      0.40      6033\n",
      "           2       0.69      0.82      0.75     10854\n",
      "\n",
      "    accuracy                           0.65     16887\n",
      "   macro avg       0.60      0.58      0.58     16887\n",
      "weighted avg       0.63      0.65      0.63     16887\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelR2, y_pred2 = train_and_evaluate_Random_Forest(feature_matrix2, labels2, X_train2, X_test2, y_train2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logistic Regression Training ---\n",
      "Accuracy for Logistic Regression: 0.6532\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.13      0.21      6033\n",
      "           2       0.66      0.94      0.78     10854\n",
      "\n",
      "    accuracy                           0.65     16887\n",
      "   macro avg       0.61      0.54      0.49     16887\n",
      "weighted avg       0.63      0.65      0.58     16887\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelL2, y_predL2 = train_and_evaluate_Logistic_Regression(feature_matrix2, labels2, X_train2, X_test2, y_train2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying on huge well-balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "5.0    36030\n",
      "4.0    18004\n",
      "1.0    16521\n",
      "3.0    15569\n",
      "2.0    13876\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataH = df[~df[\"text\"].isna()]\n",
    "dataH = dataH[dataH['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "sampled_per_rating = [dataH[dataH['rating'] == rating].sample(n=100000, random_state=42) for rating in range(1, 6)]\n",
    "required_samples = pd.concat(sampled_per_rating, axis=0)\n",
    "remaining_samples = dataH.drop(required_samples.index).sample(n=1000000 - len(required_samples), random_state=42)\n",
    "dataH = pd.concat([required_samples, remaining_samples], axis=0)\n",
    "dataH = dataH[dataH['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "print(data['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing starting ---\n",
      "---Tokenization done---\n",
      "---Numbers removed---\n",
      "---Two letter words removed---\n",
      "---Written-out numbers removed---\n",
      "---Verbs removed BIG---\n",
      "---Lematization done---\n",
      "---Stopwords removed---\n",
      "---Choosed words removed---\n",
      "---Bigrams done---\n",
      "---Preprocessing done---\n"
     ]
    }
   ],
   "source": [
    "texts_bowH, dictionaryH, id2tokenH = preprocess_text(dataH, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LDA starting ---\n",
      "--- LDA finished ---\n",
      "Medium koherence of topics: -2.3701.\n",
      "Topic 0: easy, work, pocket, thing, glass, without, set, new, best, case\n",
      "Topic 1: fit, size, buy, love, order, well, would, perfect, small, little\n",
      "Topic 2: year, old, year_old, bra, glove, awesome, purse, area, fast, especially\n",
      "Topic 3: time, come, day, band, compliment, first, many, break, strap, use\n",
      "Topic 4: stay, mask, face, hard, easily, leather, place, dry, night, plastic\n",
      "Topic 5: pair, black, foot, white, blue, boot, christmas, excellent, one, match\n",
      "Topic 6: back, earring, ring, necklace, piece, bracelet, belt, ear, nicely, chain\n",
      "Topic 7: great, love, look, good, color, nice, quality, really, make, comfortable\n",
      "Topic 8: product, review, thank, amazon, ship, seller, receive, wait, company, stylish\n"
     ]
    }
   ],
   "source": [
    "modelH, dictionaryH = LDA_training(dataH, False, 9, 1000, 2, 100, 1, 10, texts_bowH, dictionaryH, id2tokenH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_sentimentH = add_sentiment_column(dataH)\n",
    "feature_matrixH = topic_distributions_to_matrix(modelH, texts_bowH, 9)\n",
    "labelsH = data_with_sentimentH['sentiment_encoded']\n",
    "X_trainH, X_testH, y_trainH, y_testH = train_test_spliter(feature_matrixH, labelsH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Random Forest Training ---\n",
      "Accuracy for Random Forest: 0.6046\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.27      0.35     52857\n",
      "           1       0.20      0.03      0.05     27737\n",
      "           2       0.64      0.88      0.74    119406\n",
      "\n",
      "    accuracy                           0.60    200000\n",
      "   macro avg       0.44      0.40      0.38    200000\n",
      "weighted avg       0.53      0.60      0.54    200000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelRH, y_predRH = train_and_evaluate_Random_Forest(feature_matrixH, labelsH, X_trainH, X_testH, y_trainH, y_testH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logistic Regression Training ---\n",
      "Accuracy for Logistic Regression: 0.6074\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.09      0.16     52857\n",
      "           1       0.00      0.00      0.00     27737\n",
      "           2       0.61      0.98      0.75    119406\n",
      "\n",
      "    accuracy                           0.61    200000\n",
      "   macro avg       0.39      0.36      0.30    200000\n",
      "weighted avg       0.51      0.61      0.49    200000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "modelLH, y_predLH = train_and_evaluate_Logistic_Regression(feature_matrixH, labelsH, X_trainH, X_testH, y_trainH, y_testH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gradient Boosting Training ---\n",
      "Accuracy for Gradient Boosting: 0.6130\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.14      0.23     52857\n",
      "           1       0.00      0.00      0.00     27737\n",
      "           2       0.62      0.96      0.75    119406\n",
      "\n",
      "    accuracy                           0.61    200000\n",
      "   macro avg       0.39      0.37      0.33    200000\n",
      "weighted avg       0.51      0.61      0.51    200000\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "modelGH, y_predGH = train_and_evaluate_Gradient_Boosting(feature_matrixH, labelsH, X_trainH, X_testH, y_trainH, y_testH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -||- without 3 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "5.0    427346\n",
      "4.0    169729\n",
      "1.0    150564\n",
      "2.0    113742\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2H = df[~df[\"text\"].isna()]\n",
    "data2H = data2H[data2H['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "sampled_per_rating = [data2H[data2H['rating'] == rating].sample(n=100000, random_state=42) for rating in range(1, 6)]\n",
    "required_samples = pd.concat(sampled_per_rating, axis=0)\n",
    "remaining_samples = data2H.drop(required_samples.index).sample(n=1000000 - len(required_samples), random_state=42)\n",
    "data2H = pd.concat([required_samples, remaining_samples], axis=0)\n",
    "data2H = data2H[data2H['text'].apply(lambda x: len(str(x).split()) > 10)]\n",
    "data2H = data2H[data2H[\"rating\"]!=3]\n",
    "print(data2H['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing starting ---\n",
      "---Tokenization done---\n",
      "---Numbers removed---\n",
      "---Two letter words removed---\n",
      "---Written-out numbers removed---\n",
      "---Verbs removed BIG---\n",
      "---Lematization done---\n",
      "---Stopwords removed---\n",
      "---Choosed words removed---\n",
      "---Bigrams done---\n",
      "---Preprocessing done---\n"
     ]
    }
   ],
   "source": [
    "texts_bow2H, dictionary2H, id2token2H = preprocess_text(data2H, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LDA starting ---\n",
      "--- LDA finished ---\n",
      "Medium koherence of topics: -2.6863.\n",
      "Topic 0: time, day, come, use, band, easy, work, first, pocket, strap\n",
      "Topic 1: love, fit, size, buy, order, perfect, would, little, small, cute\n",
      "Topic 2: wash, ring, money, highly, highly_recommend, dry, jewelry, gold, water, green\n",
      "Topic 3: pair, year, compliment, light, old, glass, earring, son, weight, year_old\n",
      "Topic 4: back, also, right, review, enough, could, around, give, side, without\n",
      "Topic 5: great, look, well, good, nice, quality, make, really, material, price\n",
      "Topic 6: purchase, product, gift, happy, receive, item, arrive, set, thank, new\n",
      "Topic 7: color, different, black, mask, face, blue, white, sunglass, show, tie\n",
      "Topic 8: many, warm, nicely, favorite, heavy, durable, ever, excellent, neck, many_compliment\n"
     ]
    }
   ],
   "source": [
    "model2H, dictionary2H = LDA_training(data2H, False, 9, 1000, 2, 100, 1, 10, texts_bow2H, dictionary2H, id2token2H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_sentiment2H = add_sentiment_column(data2H)\n",
    "feature_matrix2H = topic_distributions_to_matrix(model2H, texts_bow2H, 9)\n",
    "labels2H = data_with_sentiment2H['sentiment_encoded']\n",
    "X_train2H, X_test2H, y_train2H, y_test2H = train_test_spliter(feature_matrix2H, labels2H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Random Forest Training ---\n",
      "Accuracy for Random Forest: 0.7146\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.31      0.40     52869\n",
      "           2       0.75      0.89      0.81    119408\n",
      "\n",
      "    accuracy                           0.71    172277\n",
      "   macro avg       0.65      0.60      0.61    172277\n",
      "weighted avg       0.69      0.71      0.69    172277\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelRH2, y_predRH2 = train_and_evaluate_Random_Forest(feature_matrix2H, labels2H, X_train2H, X_test2H, y_train2H, y_test2H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Logistic Regression Training ---\n",
      "Accuracy for Logistic Regression: 0.6986\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.08      0.14     52869\n",
      "           2       0.71      0.97      0.82    119408\n",
      "\n",
      "    accuracy                           0.70    172277\n",
      "   macro avg       0.63      0.53      0.48    172277\n",
      "weighted avg       0.66      0.70      0.61    172277\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelL2H, y_predL2H = train_and_evaluate_Logistic_Regression(feature_matrix2H, labels2H, X_train2H, X_test2H, y_train2H, y_test2H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gradient Boosting Training ---\n",
      "Accuracy for Gradient Boosting: 0.7110\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.15      0.24     52869\n",
      "           2       0.72      0.96      0.82    119408\n",
      "\n",
      "    accuracy                           0.71    172277\n",
      "   macro avg       0.67      0.55      0.53    172277\n",
      "weighted avg       0.69      0.71      0.64    172277\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelGH2, y_predGH2 = train_and_evaluate_Gradient_Boosting(feature_matrix2H, labels2H, X_train2H, X_test2H, y_train2H, y_test2H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
